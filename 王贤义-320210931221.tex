% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

% \documentclass[AutoFakeBold]{LZUThesis}
\documentclass[AutoFakeBold]{LZUThesis}
\usepackage{multirow}  % 支持表格行合并
\usepackage{booktabs}
\usepackage{svg}

\begin{document}
%=====%
%
%封皮页填写内容
%
%=====%

% 标题样式 使用 \title{{}}; 使用时必须保证至少两个外侧括号
%  如： 短标题 \title{{第一行}},  
% 	      长标题 \title{{第一行}{第二行}}
%             超长标题\tiitle{{第一行}{...}{第N行}}

\title{{面向草原生态修复的多无人机协同}{路径规划与面积分配方法研究}}

% 标题样式 使用 \entitle{{}}; 使用时必须保证至少两个外侧括号
%  如： 短标题 \entitle{{First row}},  
% 	      长标题 \entitle{{First row}{ Second row}}
%             超长标题\entitle{{First row}{...}{ Next N row}}
% 注意：  英文标题多行时 需要在开头加个空格 防止摘要标题处英语单词粘连。
\entitle{{Research on Multi-UAV Collaborative Trajectory }{ Design and Restoration Area Allocation}{ for Grassland Ecological Restoration}}
\author{王贤义}

\major{计算机科学与技术}
\advisor{焦栋斌}
\college{信息科学与工程学院}
\grade{2021级}

\maketitle

%==============================%
% ↓ ↓ ↓ 诚信说明页 授权说明书
%==============================%

% 1. 可以调整签字的宽度，现在是40
% 2. 去掉raisebox的相关注释(注意上下大括号对应)，可以改变-5那个数字调整签名和横线的上下位置

% 你的签名，signature.pdf 改为你的签名文件名，
\mysignature{
	% \raisebox{-5pt}{
	\includegraphics[width=40pt]{signature.pdf}
	% }
}
% 你手写的日期，signature.pdf 改为你的手写的日期文件名
\mytime{
	% \raisebox{-5pt}{
	\includegraphics[width=40pt]{signature.pdf}
	% }
}
% 老师的手写签名，signature.pdf 改为老师的手写签名文件名
\supervisorsignature{
	% \raisebox{-5pt}{
	\includegraphics[width=40pt]{signature.pdf}
	% }
}
% 老师手写的时间，signature.pdf 改为老师的手写的日期文件名
\teachertime{
	% \raisebox{-5pSt}{
	\includegraphics[width=40pt]{signature.pdf}
	% }
}
% 老师手写的成绩
\recommendedgrade{
	% % \raisebox{-5pt}{
	\includegraphics[width=40pt]{signature.pdf}
	% % }
	% % \raisebox{-5pt}{
	% \includegraphics[width=40pt]{signature.pdf}
	% % }
	% 手写成绩
}

\makestatement

%==============================%
% ↑ ↑ ↑ 诚信说明页 授权说明书
%==============================%


%=====%
%论文（设计）成绩：注意2007的模板要求，成绩页在最后，2021要求成绩页在摘要前面
%=====%

% 下面这些注释掉可以去掉成绩、评语什么的
\supervisorcomment{研究工作中，王贤义同学表现出了扎实的专业基础、良好的科研能力和创新思维。他能够独立思考问题，积极探索解决方案，并在算法设计和实验验证中付出了大量努力。论文写作规范，结构清晰，内容翔实，理论与实践相结合，充分体现了作者的学术素养。

	本论文的研究成果对于提升草原生态修复效率具有重要参考价值，也为无人机协同任务优化领域提供了新的思路。建议作者在未来研究中进一步探索算法在更复杂环境下的适应性，以及与实际应用场景的结合。

	综上所述，该论文达到了本科毕业论文的要求。
}


\committeecomment{}

\finalgrade{}
% 0上面这些注释掉可以去掉成绩、评语什么的


\frontmatter



%中文摘要
\ZhAbstract{本文系统研究了多无人机协同草原修复面积最大化问题。草原生态系统作为维持全球
	生物多样性和生态平衡的重要组成部分，近年来却因人类活动和气候变化等多重因素面临
	严重退化，表现为植被覆盖率下降、土壤沙化加剧等多方面生态失衡。传统的草原修复方
	法主要依赖人工作业和机械化手段，存在效率低、成本高、难以适应大范围复杂地形等局
	限性。随着无人机技术的快速发展，多无人机协同作业为草原生态修复提供了新的解决思
	路，但如何在能量受限、任务复杂的环境下实现无人机协同与修复面积最大化，仍是亟需
	突破的难题。

	针对上述问题，本文提出了一种基于深度强化学习的多无人机协同草原修复优化方法。
	首先，将待修复草原建模为一个完全无向图，节点代表待修复区域，每个区域包含位置坐
	标、退化程度和面积等信息。综合考虑无人机在飞行、播种和航拍过程中的能量消耗模型，
	以及草种重量动态变化对能耗的影响，建立了以修复面积最大化为目标的优化模型。本文
	创新性地采用深度强化学习框架，利用 Transformer 作为编码器提取静态环境特征和动态无
	人机状态特征，结合指针网络作为自回归解码器，动态构建无人机修复路径和任务分配方
	案。通过设计兼顾能量约束与修复效率的奖励函数，并采用 Actor-Critic 算法进行训练，使
	模型能够自主学习并优化修复策略。

	在此基础上，本文进一步设计了多无人机协同调度算法，通过地面信息中心实时整合
	各无人机的状态信息，动态调整修复地图，实现多无人机间的高效协同与任务再分配，从
	而提升整体修复效率。实验部分在多种规模和分布的草原实例上进行了仿真验证，结果表
	明，本文提出的算法在修复面积、路径优化和能量利用率等方面均优于传统优化方法，修
	复面积提升幅度最高可达 40%，展现出良好的泛化能力和实际应用前景。综上，本文为大
	规模草原生态修复任务提供了一种高效、智能的技术方案，对无人机协同作业和生态环境
	治理具有重要的理论意义和应用价值。}{草原生态修复；多无人机协同；Transformer；指针网络；Actor-Critic 算法}


%英文摘要
\EnAbstract{This paper investigates the problem of maximizing restoration area through multi-UAV coordination in grassland rehabilitation. Grassland ecosystems play a crucial role in maintaining
	biodiversity and ecological balance, but grassland degradation has become increasingly serious in
	recent years. To address the inefficiency of traditional restoration methods in large-scale environments, this research proposes a deep reinforcement learning-based optimization method for multi-UAV cooperative grassland restoration.

	We first model the grassland to be restored as a complete undirected
	graph, where each node represents a restoration area with information on location coordinates,
	degradation degree, and area size. Considering the energy consumption models of UAVs during
	flight, seeding, and aerial photography, as well as the dynamic impact of seed weight on energy
	consumption, we formulate an optimization model aimed at maximizing restoration area. The proposed method is built on a deep reinforcement learning framework that employs Transformer as an
	encoder to extract static environmental features and dynamic UAV status features, combined with a
	pointer network as an autoregressive decoder to construct solution schemes. By designing a reward
	function that accounts for energy constraints and applying the Actor-Critic algorithm for training,
	the model can autonomously learn optimal restoration strategies.

	On this basis, we also design a
	multi-UAV coordination scheduling algorithm that dynamically adjusts restoration maps through
	a ground information center that integrates information from all UAVs, maximizing restoration
	efficiency. Experimental results show that compared with traditional optimization methods, our
	proposed algorithm demonstrates superior performance in grassland restoration scenarios of various scales, with restoration area improvements of up to 40\%, providing an efficient and intelligent
	technical solution for grassland ecological restoration.}{UAV coordination, grassland restoration, area maximization, deep reinforcement learning, Transformer, pointer network, Actor-Critic algorithm}

%生成目录
% \tableofcontents
% 下面这个包含图表目录
\customcontent


% % 部分同学需要专业术语注释表，* 表示不加入目录
% \chapter*{专业术语注释表}
% \begin{longtable}{lll}
%   \caption*{缩略词说明}\\
%   SS & Spread Spectrum & 扩展频谱 \\
%   PAPR & Peak to Average Power Ratio & 峰均比\\
%   DCSK & Differential Chaos Shift Keying &差分混移位键控\\
%   dasd & fdhfudw eqwrqw fasfasfs fewev wqfwefew &\tabincell{l}{太长了\\换行一下}\\
% \end{longtable}


% %文章主体
\mainmatter

\chapter{\texorpdfstring{绪 \quad 论}{绪论}}

% ！学校要求的规范，绪论是单独的，不是第一章，但是老师们都是让作为第一章，这里我把它放在了论文里，如果你要让在外面，只需要把上面的 \mainmatter 这一句话放在“绪论内容后面，正文第一章前面”即可，也就是 \chapter{latex部分用法简介} 这一句话上面

% 这里是绪论，也可以说是引言，在LZUThesis.clc里面改，引言写什么呢，先凑字数，

% 注意啊，段落在latex里面是要空一行的，不要简单一个回车，编译过程中警告内容无需管

\section{研究背景与意义}

草原生态系统在全球环境保护和可持续发展中发挥着至关重要的作用。作为陆地生态系统的重要组成部分，草原面积约占全球陆地总面积的 26\% 至 40\%\cite{chapin2013global}，承担着防风固沙、涵养水源、调节气候、美化环境和维持生物多样性等多重生态功能\cite{gibson2009grasses}。然而，近年来，全球草原退化问题日益严重，主要受到人类活动和气候变化的双重影响，表现为植被覆盖率下降、土壤沙化、水资源减少等多方面的生态失衡\cite{bai2020long}。这些退化不仅削弱了草原的生态服务功能，还对畜牧业生产和区域经济发展造成了不利影响。因此，高效、精准地实施草原修复措施，以恢复其生态功能，已成为当前环境治理领域亟待解决的关键问题之一\cite{freitag2021restoration}。

草原生态系统的健康状况直接关系到全球气候变化的缓解、生物多样性的维护以及区域经济的可持续发展。研究表明，草原作为重要的碳汇，在碳循环和气候调节方面具有不可替代的作用\cite{reinermann2020remote,dass2018grasslands}。同时，草原退化导致的生态系统服务功能下降，也对农牧业生产和当地居民生计产生了严重影响。因此，开展草原生态修复研究不仅具有重要的环境保护意义，也具有显著的社会经济价值。

在草原修复方法方面，传统手段主要依赖人工植被恢复、机械作业及定点喷洒草籽等方式。这些传统人工方法通常耗费大量人力物力，且难以在大范围草原环境下进行高效修复。机械化手段虽然提高了作业效率，但在复杂地形和偏远区域的应用受到诸多限制。近年来，无人机（Unmanned Aerial Vehicles, UAV）技术的快速发展为草原修复提供了一种全新的解决方案。无人机凭借其机动性强、作业成本低、可远程操作等优势，在草原生态监测、病虫害防治和生态修复等领域得到了广泛应用\cite{steffen2015planetary}。然而，无人机在实际修复任务中仍面临诸多挑战，主要受到电池续航能力、载荷能力和复杂地形的限制。特别是如何在有限的能量和时间内覆盖更大的修复区域，以及如何优化无人机的路径规划以提高修复效率，成为亟需解决的技术难题。

\section{国内外研究现状}

在无人机辅助草原修复领域，国内外研究主要集中在两方面：无人机技术在生态监测与修复中的应用，以及路径优化与任务分配算法研究。国际上，Nex等人\cite{nex2014uav}综述了无人机在三维地图绘制和环境监测的应用优势，Mohan等人\cite{mohan2021uav}和Sun等人\cite{sun2018unmanned}将无人机技术应用于森林、草原等生态系统监测。Zeng等人\cite{zeng2017energy}针对能量效率问题，研究了基于轨迹优化的无人机通信系统，为能源受限条件下的任务执行提供了新思路。

国内无人机技术在草原修复中也取得一定进展，特别是在播种、喷洒\cite{faiccal2017adaptive}和生态监测等应用场景。路径优化方面，传统方法主要基于旅行商问题和定向问题\cite{shivgan2020energy}，通过最优路径搜索减少飞行距离，提高执行效率。针对无人机巡逻、覆盖等任务，研究者提出了多种优化算法\cite{rajan2022routing,yang2015path,guo2021spraying}。人工智能方法虽在制造系统优化等领域有广泛应用\cite{renzi2014review}，但在草原修复领域的应用尚待深入探索。

在无人机播种系统方面，Elliott\cite{elliott2016potential}探索了低成本无人机在热带森林生态系统中通过空中播种实现加速自然再生的可能性。Pedersen等人\cite{pedersen2017robotic}则利用无人机创建再播种地图，而非直接用于作物生产。Huang等人\cite{huang2020design}提出了一种特殊的油菜籽无人机播种系统，采用微型气助式集中计量装置，提高了播种效率与精度。Faiccal等人\cite{faiccal2017adaptive}开发了一种能自适应规划飞行路线的系统，确保精确将农药沉积在目标区域。Guo等人\cite{guo2021spraying}提出了喷洒无人机的喷雾分布模型和覆盖路径规划方法，仿真结果显示了其有效性。然而，这些研究主要基于规则田地地图，且未充分考虑无人机的能量消耗，也很少将路径规划与播种任务结合考虑，可能与实际应用场景存在差异。

在无人机路径规划研究中，研究者主要关注高效区域覆盖算法以延长无人机作业时间。Vasisht等人\cite{vasisht2017farmbeats}设计了一种新颖的路径规划算法，充分利用风力辅助加速和减速，延长无人机操作时间并最小化覆盖给定区域所需的时间。Palomino等人\cite{palomino2019towards}提出了与路径规划过程相关的自动化工作流算法，该算法不仅能识别工作空间，还能在农业监督活动中处理路径规划。Shivgan等人\cite{shivgan2020energy}研究了无人机在环境传感和测量应用中完成任务的路径规划问题，将问题建模为旅行商问题(TSP)，通过优化无人机能量来处理无人机的飞行时间限制。Rossello等人\cite{rossello2021information}为精准农业开发了一种新型路径规划算法，覆盖大规模区域，考虑了飞行时间约束并最大化系统状态的估计质量，将问题建模为特殊的定向问题(OP)，即混合整数半定规划(MISDP)，通过启发式算法求解。

智能优化算法方面，Feng等人探索了进化优化与迁移学习在复杂路径问题中的应用\cite{feng2015memes}，Rossello等人提出了基于信息驱动的路径规划方法\cite{rossello2021information}。多层次优化方法被用于任务分配与路径优化\cite{colson2007overview}，协同优化与分布式智能算法则应用于大规模组合优化问题\cite{huang2004cooperative}。

在两阶段优化方法研究方面，Shen等人\cite{shen2009two}考虑了大规模生物恐怖主义紧急情况下的两阶段车辆路径问题。Li等人\cite{li2010inexact}开发了一个不精确两阶段水管理模型，用于在不确定条件下规划农业灌溉。Azadeh等人\cite{azadeh2019two}提出了一种两阶段路径优化方法，限制飞行边界并减少轻型飞机运输系统中的变量和约束数量。Maini等人\cite{maini2019cooperative}设计了一种两阶段策略，为覆盖应用中燃料约束下的协同空中-地面车辆路径规划问题寻找高效解决方案。Rajan等人\cite{rajan2022routing}为数据收集任务场景中的无人机路径规划问题制定了两阶段随机规划模型。虽然上述研究主要采用两阶段方法处理路径规划问题，但大多数研究仅根据问题过程将其机械分解为两个单独解决的阶段，而没有考虑它们之间的耦合关系。

此外，双层规划问题\cite{colson2007overview}是另一种两阶段优化问题。无人机草原修复问题与双层规划问题相关，它考虑了优化问题两个阶段之间的耦合关系。Angelo等人\cite{angelo2015study}研究了双层生产-分配规划问题。Sinha等人\cite{sinha2017review}全面回顾了从基本原理到解决方案的双层优化，包括经典和进化方法。然而，这些方法难以直接解决本研究中的问题，因为它们未考虑无人机草原修复问题的特定特性。

然而，传统方法在面对动态环境、复杂地形和多目标任务时仍存局限性\cite{gann2019international}，通常基于静态环境优化，而实际应用常受风速、温度、地形等因素影响\cite{blackburn2021monitoring}。单架无人机作业能力有限，难以满足大规模修复需求，因此多无人机协同作业成为提高效率的重要研究方向\cite{dorling2016vehicle,maini2019cooperative}。虽已有研究探索多无人机系统的协同控制\cite{buters2019seed}，但专门针对草原修复的多无人机协同优化研究仍较缺乏，特别是考虑能量约束、播种动态负载变化及修复面积最大化等多重目标的综合优化问题。

\section{研究内容}
本文旨在解决草原生态修复过程中面临的高效率覆盖与资源有限的矛盾问题。针对传统修复方法在大规模、复杂地形下的局限性，提出了一种基于深度强化学习的多无人机协同草原修复优化模型。研究采用"单机智能、多机协同"的分层设计思路：首先，利用强化学习算法训练单架无人机在复杂环境中的智能决策能力，使其能够自主学习并优化修复策略；其次，在此基础上构建多无人机协同作业的优化框架，实现在能源有限约束下的任务动态分配与路径协同优化。这种层次化的智能决策架构不仅能够根据草原退化状况、地形特征和无人机状态自适应调整修复策略，还能在多无人机间建立高效的信息交互与任务协调机制，从而在最大化修复面积的同时，提高了系统对复杂动态环境的适应性和鲁棒性。
% \section{二级标题}

% 绪论其实也可以有二级标题，要不然，论文要求：“包括毕业论文（设计）的研究目的、意义、范围、研究设想、方法、实验设计、选题依据等；还包括毕业论文（设计）研究领域的历史回顾，文献追溯，理论分析等内容”全部写成一堆不成？

% 测试公式第二章继续编号，而不是从1开始
% \begin{equation}
% 	E=mc^2
% \end{equation}
% \mainmatter

\chapter{多无人机协同的草原修复面积最大化模型}

本文将待修复草原实例定义为一个完全无向图 $G = (V, E)$，其中 $V = \{v_0, v_1, ..., v_N \}$ 表示所有带修复区域的集合, 每个区域 $v_i$ 具有位置坐标、退化程度和待修复区域面积等信息，其中区域 $v_0$ 定义为地面信息融合中心，退化程度和待修复面积均为 0。$E = (e_{ij} |i, j \in V, i = j)$ 表示所有边的集合，每条边 $e_{ij}$ 仅有边长这一属性。无人机 (多旋翼) 起飞时电池能量定义为 $E_{max}$, 携带的草种重量为 $Q$。根据国际生态修复实践原则和标准\cite{gann2019international} ，本文将待修复草原的退化程度范围定为 [0.3, 0.8]。

\begin{figure}[htbp]
	\centering
	\includesvg[width=1\textwidth]{figures/多无人机修复退化区域实例.svg}
	\caption{无人机草原修复区域示意图。图中展示了待修复区域的分布情况，每个圆点代表一个待修复区域，区域大小和颜色深浅表示其退化程度和面积。}
	\label{fig:restored-areas}
\end{figure}

针对待修复草原实例，本文的目标是在无人机从基站出发并在为每个待修复区域提供服务前耗尽能量的情况下，最大化无人机修复的总面积 (假设仅当无人机飞行至待修复区域时才能进行播种)。由于无人机在播撒草籽修复过程中自身重量不断变化，其飞行能耗又与之直接相关，如何权衡无人机飞行能耗与修复能耗之间的关系时问题的核心所在。在修复过程中，无人机的能量消耗主要包括三个部分：无人机在待修复区域中播种的能量消耗、无人机进行航拍的能量消耗以及无人机在飞行过程中的能量消耗。相应地，这三部分能量消耗可以分别表示为 $E_s$, $E_{ap}$ 和 $E_f$。此外，无人机的能量消耗恒定速度下的载荷重量成正比，Dorling\cite{dorling2016vehicle} 等推导出 $h$ 转子无人机的功耗方程，表示如下：


\begin{equation} \label{approximation-power-consumption}
	%\begin{small}
	P(\bar{q}_{ij}) = (M + \bar{q}_{ij})^{\frac{3}{2}}\sqrt{\frac{g^3}{2 \rho \varsigma h}},
	%\end{small}
\end{equation}
其中，$M = W + m$，$W$ 表示无人机框架重量，$m$ 表示其电池重量。$q_{ij}$ 表示无人机当前载荷重量（草籽重量），$g$ 为标准重力加速度，$\rho$ 表示空气密度，$\zeta$ 表示无人机旋转叶片盘面积，$h$ 表示无人机旋翼数量，上述参数单位均参考国际单位制标准。为简化问题，本文假设无人机在待修复区域间以固定高度、恒定速度飞行，同时忽略不同天气条件对无人机飞行的影响，如温度、风力、雨量和沙尘暴等。此外，考虑到无人机能量的限制，本文假设所有草原上的退化区域均可在一次修复过程中由一架无人机修复 (无论是否完全修复)。



\section{无人机能量消耗模型}

\subsection{飞行能耗}

无人机飞行期间的能耗，即单架无人机从基站出发，根据修复方案遍历带修复区域最后返回基站的飞行路径能耗。本文设 $x_{ij} \in \{0,1\}$ 为 $0$-$1$ 决策变量，定义如下：

\begin{equation}
	%\begin{small}
	x_{ij} =
	\begin{cases}
		1, & \mbox{$(v_i,v_j)$ is covered in the tour,} \\
		0, & \mbox{otherwise}.
	\end{cases}
	%\end{small}
\end{equation}

本文假定无人机在固定高度以恒定速度 $v$ 飞行，每单位距离的能耗相同。因此修复过程中无人机飞行的总能耗可以表示

\begin{equation} \label{flight-energy}
	\begin{scriptsize}
		E_f =  \sum^N_{i=0}\sum^N_{j \neq i} e^f_{ij} d_{ij} x_{ij},
	\end{scriptsize}
\end{equation}
其中，$e^f_{ij}$ 表示无人机的飞行单位距离的能耗，与无人机在待修复区域 $v_i$ 和 $v_j$ 之间携带的种子重量 $\bar{q}_{ij}$ 相关，可由公式 (1) 实时计算得出。$\bar{q}_{ij}$ 表示从待修复区域 $v_i$ 和 $v_j$ 中无人机携带的草种重量，满足以下条件：

\begin{equation} \label{remained-weight}
	\begin{scriptsize}
		\sum^N_{j=0, i\neq j}\bar{q}_{ji} -  \sum^N_{j=0, i\neq j}\bar{q}_{ij} = Q_i, \forall i \in V_a,
	\end{scriptsize}
\end{equation}
\begin{equation} \label{demanded-weight}
	\begin{small}
		\bar{q}_{ij} \leq Q x_{ij}, \forall (i,j) \in A.
	\end{small}
\end{equation}

\subsection{播种能耗}

本文将每个待修复区域离散为 $c_i(i = 1, ..., n)$ 个单位圆面积。无人机播种的能量消耗可视为与草地退化程度和其携带种子的重量相关的函数。无人机每单位播种面积的能耗可以定义为：$e_i = \eta q_i$。其中 $\eta$ 是一个与能量消耗相关的正参数，$q_i$ 是第 $i$ 个待修复区域每单位圆修复所需种子的重量。进一步地，修复每个区域单位圆面积所需的种子重量可视为该区域的草地退化程度的函数\cite{klaus2017enriching}：$q_i = (1 + l_i) \gamma$。其中，$l_i \in [0.3, 0.8]$ 表示第 $i$ 个待修复区域的草地退化程度，$\gamma$是与草地环境有关的正参数。综上所述，无人机 $u$ 在第 $i$ 个待修复区域中修复 $\sigma_i$ 个单位圆所需的草种重量可以表示为$Q^u_i = \sigma_i q_i$，$\sigma_i$ 表示无人机修复的单位圆数量 $(1\leq\sigma_i\leq c_i)$。因此，无人机 $u$ 在待修复区域内播种的总能量消耗可以表示为：

\begin{equation} \label{seeding-energy}
	\begin{small}
		E_s = \sum^N_{i = 1}\sum^N_{j \neq i}\sigma_i e_i x_{ij},
	\end{small}
\end{equation}
其中，二元 $0-1$ 变量 $x_{ij}$ 用于确定无人机是否将对待修复区域 $v_j$ 进行修复。

\subsection{信息采集能耗}

除飞行能耗以外，无人机还在各待修复区域收集修复信息。无人机搭载高光谱相机进行航拍的总能耗可表示为：

\begin{equation} \label{hyper-spectral-camera-energy}
	\begin{scriptsize}
		E_{ap} = e_{ap}\sum^N_{i=1}\sum^N_{j \neq i} x_{ij}\sigma_i,
	\end{scriptsize}
\end{equation}
其中，$e_{ap}$ 表示无人机在 $\sigma_i$ 个被修复的单位圆中航拍所消耗的能量。

\section{优化目标}

由于能量有限，无人机在一次修复过程中无法覆盖所有退化区域。为实现更高效的修复效果，本文在优化目标中引入了权重系数，使无人机优先修复退化程度更高的区域。具体地，优化目标定义为：

\begin{equation} \label{maximize-weighted-sum}
	\begin{small}
		C = \sum_{i=1}^{N} (l_i + 0.7) \cdot \sigma_i
	\end{small}
\end{equation}

其中，$l_i$ 表示第 $i$ 个区域的退化程度，$\sigma_i$ 为无人机在该区域修复的面积，$(l_i - 0.3)$ 作为权重因子。这样设计的原因在于，单纯最大化修复面积会导致模型难以收敛且忽略生态优先级，通过引入退化程度权重，能够引导模型优先修复生态状况更差的区域，提高整体修复的生态效益和模型收敛速度。

\section{数学模型}

综上所述，本文考虑在无人机的最大能量限制 $E_{max}$ 下，对待修复区域的播种修复和航拍能耗 $E_s$ 和 $E_{ap}$、待修复区域间的飞行能耗 $E_f$ 以及无人机的飞行轨迹 $x_{ij}$ 进行组合优化。无人机通过将自身决策与彼此信息实时交流从而实现协同 (修复地图再分配等)。多无人机协同的草原修复模型可被描述为：

\begin{subequations}\label{GRP}
	%\begin{small}
	\begin{align}
		\max_{x_{ij},\sigma_i} \quad & C = \sum^N_{i = 1}\sum^N_{j \neq i} x_{ij}\sigma_i                                                                                                          \\
		\emph{s.t.} \quad            & \sum^N_{i = 1}\sum^N_{j \neq i}\sigma_i e_i x_{ij} + e_{ap}\sum^N_{i=1}\sum^N_{j \neq i} x_{ij}\sigma_i \nonumber                                           \\
									 & + \sum^N_{i=0}\sum^N_{j \neq i} (M + \bar{q}_{ij})^{\frac{3}{2}}\sqrt{\frac{g^3}{2 \rho \varsigma h}} x_{ij} d_{ij} \leq E_{max}, \label{energy-constraint} \\
									 & \sum^N_{i=1, i\neq j} \sigma_i q_i x_{ij} \leq Q, \forall j \in V_a,\label{total-load}                                                                      \\
									 & \sum^N_{j=0, i\neq j}\bar{q}_{ji} -  \sum^N_{j=0, i\neq j}\bar{q}_{ij} = \sigma_i q_i, \label{carry-load} \quad \forall i \in V_a,                          \\
									 & \bar{q}_{ij} \leq Q x_{ij}, \quad \forall (i,j) \in A, \label{load}                                                                                         \\
									 & \sum^N_{i=0, i\neq j} x_{ij} = \sum^N_{j=0,i\neq j} x_{ij} = 1, \quad \forall i,j \in V_a, \label{enter-point}                                              \\
									 & \sum^N_{j=1} x_{0j} = \sum^N_{j=1} x_{j0} = 1, \label{start-point}                                                                                          \\
									 & x_{ij} \in \{0,1\}, \label{binary-variable}                                                                                                                 \\
									 & 1 \leq \sigma_i \leq c_i, \label{area-constraint}                                                                                                           \\
									 & \bar{q}_{ij} \geq 0. \label{flight-weight}
	\end{align}
	%\end{small}
\end{subequations}

上述优化模型中，约束条件\eqref{energy-constraint}确保无人机在一次调度周期内的能量消耗不超过其最大能量容量$E_{max}$。约束\eqref{total-load}保证无人机携带的草种重量$Q$必须在无人机返回基站前全部播撒完毕。约束\eqref{carry-load}表示无人机在服务完一个修复区域后草种重量的减少，并与该区域的需求修复面积相匹配，同时消除任何非法的子回路。约束\eqref{load}保证修复区域$v_j$的草种需求不超过无人机携带的剩余草种重量。约束\eqref{enter-point}确保无人机最多进入每个修复区域一次，并在播种后离开该区域。约束\eqref{start-point}保证无人机的路径始于并终于基站。约束\eqref{binary-variable}确保二元变量取整数值。约束\eqref{area-constraint}确保待修复的单位圆数量不超过其最大区域。约束\eqref{flight-weight}为非负性约束。综合而言，该问题是一个多变量组合优化问题，变量包括二元变量$x_{ij}$和整数变量$\sigma_i$，因此直接使用传统优化方法求解较为困难。


\section{本章小结}
本章提出并形式化了无人机修复模型，重点分析了飞行、播种、航拍三种能耗，并以修复面积最大化为目标，形成了多变量组合优化问题，为后续强化学习求解奠定基础。

% Constraints \eqref{energy-constraint} ensure that the energy consumption in a scheduling cycle of UAV cannot exceed its energy capacity $E_{max}$. Constraints \eqref{total-load} impose that the weight of the grass seeds $Q$ carried by UAV must be sowed before the UAV returns to the base station. Constraints \eqref{carry-load} indicate the reduced weight of grass seeds for UAV after it severs a restored area and equaling the demanded restored area, and also eliminate any illegal subrours. Constraints \eqref{load} guarantee that the demanded grass seeds at the restored area $v_j$ cannot exceed the weight of remaining grass seeds carried by the UAV. Constraints \eqref{enter-point}
% ensure that the UAV enters each restored area at most once and leaves the restored area after seeding.
% Constraints \eqref{start-point}
% guarantee that the UAV begins and ends its route at the base station. Constraint \eqref{binary-variable} ensures that the binary variables value are integers. Constraint \eqref{area-constraint}
% ensures that the number of unit circle to be restored cannot exceed its maximum areas. Constraint \eqref{flight-weight} is  nonnegativity restrictions.

% We can observe that the optimization problem \eqref{GRP} is a multi-variable combinatorial optimization problem, since the set of feasible solutions is discrete in terms of the binary variable $x_{ij}$ and the integer variable $\sigma_i$.
% Therefore, it is difficult to directly solve the optimization problem \eqref{GRP} by using the traditional optimization methods. By further analyzing the optimization problem \eqref{GRP}, we can find the following characteristics. First, the size of restoration areas and the amount of seeding in each restored area may vary with different service order by UAV depending on the UAV's flight trajectory. Second, the size of restoration areas and the amount of seeding in each restored area will affect the next flight trajectory of UAV. In brief, the trajectory design of UAV, the size of restoration areas, and the amount of seeding are closely coupled. If the optimization problem \eqref{GRP} is solved directly, the trajectory design of UAV, the size of restoration areas, and the amount of seeding are generated separately, resulting in that their dependence is ignored unreasonably. Therefore, there exist two challenges for solving effectively the optimization problem \eqref{GRP}.

\chapter{基于强化学习的最优修复方案}

\section{马尔可夫过程建模}
本文将修复过程中的每架无人机视为一个智能体，将待修复草原视为以欧几里得距离为边权值的无向全联通图。对单架无人机而言，其部分修复过程可视为从某点出发，遍历修复地图并完成相应修复任务，最后返回起点。

在无人机的修复过程中，由于其在两块相邻修复区域间的飞行能耗正比于此时无人机自身重量与路径长度，而自身重量又随着播散种子修复区域的过程动态变化，因而其总飞行能耗与总飞行长度、修复区域的顺序及修复面积均有复杂关系。换言之，作为优化目标的总修复面积由于与无人机耗能而与其飞行路径间接相关，因而二者之间存在复杂的耦合关系，如何解决修复面积的最优化，毫无疑问是一个困难的 $\text{NP-hard}$ 问题。

针对该难题，本文拟采用强化学习方法学习这一复杂的函数关系，具体建模如下：

\subsection{状态}

本文将一个待修复草原实例设为$V=\{v_i\}_{i=0}^n$,其中包含了各个待修复区域的位置、退化程度等各项信息。各点以$v_0$为起始点按照某种策略$\pi$，逐渐完善部分解$\{(v_0,a_0),(v_i,a_i)\}_{i=1}^{uav_{now}}$。其中，编号$i$表示无人机访问各点的次序，$uav_{now}$为无人机已经过的点数量,$a_i$代表按照策略$\pi$在点$v_i$修复的面积。本文将该部分解作为智能体在强化学习中的状态，记作$s(<i),i\in[1,n+1]$。初始状态记作$s(<1)=v_0$，表示无人机处于地面中心；结束状态记作$s(<n+1)=s$，表示无人机已访问全部点。

\subsection{动作}

如上所述，无人机状态是修复过程的部分解，因而本文将动作设置为无人机修复过程某一步的解，即将无人机在状态$s(<i)$时的动作记作$\pi_i=s(i)$,$i\in[1,n+1]$，即可实现强化学习中动作-状态对的自然更新。

\subsection{状态}

转移函数如上所述，在动作选取完毕后无人机新的状态也被唯一确定，因而状态转移函数具有完全确定性，记其为$S(s(<i)|s(i))=s(<i+1)$,$i\in[1,n]$。

\subsection{策略函数}

如上所述，草原修复过程被创建为一个马尔可夫决策过程，因而其策略函数可被链式分解为：$p(\pi|s)=\prod_{i=1}^n p(s(i)|s(<i))$(10)其中，$p(s(i)|s(<i))$表示智能体在状态$s(<i)$下选择动作$s(i)$的概率。

\subsection{奖励函数}

作为强化学习方法的核心之一，奖励函数的设计必须考虑全面，以防止模型难以收敛。经本文实验证明，单纯以最基本的修复面积作为模型的奖励函数会使得模型收敛速度大大减缓，同时考虑到无人机自身能量限制引入的惩罚项，本文最终将一组解的奖励函数设置为：
\begin{equation}
	R(\pi|V)=\alpha_{p}*Pel+\alpha_{r}*\sum_{i=1}^{n}(l_i+0.7)*a_{i}
	\label{eq:11}
\end{equation}

\begin{equation}
	Pel=\left\{\begin{array}{ll}
		d_{n}^{0}+\sum\limits_{i=1}^{n}d_{i}^{i+1} & E_{rest}<0     \\
		0                                          & E_{rest}\geq 0
	\end{array}\right.
	\label{eq:12}
\end{equation}
上式中，$\alpha_p$, $\alpha_r$为修正系数，$l_i$为第$i$个区域的退化程度，$Pel$为控制无人机能量约束的惩罚项。通过引入$(l_i-0.3)$作为权重因子，模型可以优先修复退化程度更高的区域，从而实现对草原生态健康状况的精准干预。同时为了加快模型收敛速度，本文将不符合无人机能量约束时的惩罚项设置为其从起点到返回地面中心经历的路径长度，以确保在模型收敛的前期尽可能获得的解路径长度较短、符合能量约束，并在此前提下进行修复面积的决策。

\section{神经网络模型构建}

对于序列决策问题，编码器-解码器模型\cite{vaswani2017attention}作为最经典的神经网络模型结构之一，取得了优异的效果。本文采用稍加修改的经典的Tranformer作为编码器，指针网络\cite{vinyals2015pointer}作为自回归解码器以实现构造式求解。

\subsection{特征提取与模型架构}

本文对待求解的待修复草原模型提取两部分特征：待修复区域的坐标、退化程度等静态特征，及无人机自身剩余能量、剩余重量等动态特征。模型参数用 $\theta \in \Theta$ 表示，包含编码器和解码器的所有可学习参数。

模型初始化过程如算法\ref{alg:grp_main_training}的第1-2行所示：
\begin{equation}
	M_{\theta} \leftarrow \text{InitializeModel}(d_e, d_h, n_{layers}, n_{heads}, \theta),
\end{equation}
其中，$d_e$ 为嵌入维度，$d_h$ 为隐藏层维度，$n_{layers}$ 为编码器层数，$n_{heads}$ 为注意力头数量。同时，本文初始化基线模型 $B_{model}$ 用于估计值函数，支持Actor-Critic算法的训练。

\subsection{编码器结构}

参考了Bresson\cite{bresson2021transformer}等人的方法，编码器由 $n_{layers}$ 层Transformer层堆叠而成，采用BatchNorm归一化，表示为：

\begin{equation}
	h^{l=0} = h^{in} W^{in} \in \mathbb{R}^{n \times d_e}
	\label{eq:13a}
\end{equation}

\begin{equation}
	h_{rc}^{l+1} = \text{BN}\left(\text{MHA}^{l+1}(h^l) + h^l\right)
	\label{eq:13b}
\end{equation}

\begin{equation}
	h^{l+1} = \text{BN}\left(\text{Relu}\left(h_{rc}^{l+1} W_1^{l+1}\right) W_2^{l+1} + h_{rc}^{l+1}\right)
	\label{eq:13c}
\end{equation}

编码器同时处理静态环境特征和动态无人机状态特征，生成用于解码器的上下文表示。

\subsection{解码器结构}

自回归解码器通过循环解码逐步构造解决方案。解码器接收编码器的输出，其工作流程为：

\begin{equation}
	rm^{i}, h_{gru}^{i} =
	\left\{
	\begin{array}{ll}
		\text{GRU}(h_{0}^{i}, 0)               & i = 0 \\
		\text{GRU}(h_{i-1}^{i}, h_{gru}^{i-1}) & i > 0,
	\end{array}
	\right.
	\label{eq:14}
\end{equation}
其中，$0$为全零向量，GRU(gated recurrent unit)为循环神经网络更新解嵌入。解码器通过注意力机制选择下一个待修复区域及修复面积：

\begin{equation}
	h_{p}^{l=i} = \text{Attention}(\text{static}, \text{dynamic}^{i}, mn^{i})
	\label{eq:15a}
\end{equation}

\begin{equation}
	\mu_{i} =
	\begin{cases}
		C \times \text{Tanh}\left((W_{q}h_{p}^{l=i})^{T}(W_{k}h_{p}^{l=i})\right) & x_{i} \notin \pi_{i} \\
		-\infty                                                                   & \text{otherwise}
	\end{cases}
	\label{eq:15b}
\end{equation}

\begin{equation}
	p(i|s_{i}) = \text{Softmax}(\mu_{i})
	\label{eq:15c}
\end{equation}

在模型前向传播中，如算法\ref{alg:grp_batch_training}第4行所示，解码器生成成本值 $C$ 和对数概率 $\log p$：

\begin{equation}
	(C, \log p) \leftarrow M_{\theta}(x),
\end{equation}
其中 $C$ 表示当前策略的成本（负奖励），$\log p$ 表示选择各动作的对数概率。

\subsection{训练流程}
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\caption{GRP主训练流程}
		\label{alg:grp_main_training}
		\Require $\theta \in \Theta$（模型参数），$d_e \in \mathbb{N}$（嵌入维度），$d_h \in \mathbb{N}$（隐藏层维度），$n_{layers} \in \mathbb{N}$（编码器层数），$n_{heads} \in \mathbb{N}$（注意力头数），$\alpha \in \mathbb{R}^{+}$（学习率），$N_{epoch} \in \mathbb{N}$（训练轮数），$B \in \mathbb{N}$（批次大小），$G_{size} \in \mathbb{N}$（图规模）
		\Ensure 最优参数 $\theta^* \in \Theta$
		\State $M_{\theta} \leftarrow \text{初始化模型}(d_e, d_h, n_{layers}, n_{heads}, \theta)$
		\State $B_{model} \leftarrow \text{初始化基线模型}()$
		\State $\mathcal{O} \leftarrow \text{Adam优化器}(\{M_{\theta}, B_{model}\}, \alpha)$
		\State $\mathcal{S} \leftarrow \text{学习率调度器}(\alpha)$
		\State $\mathcal{D}_{val} \leftarrow \{x_i\}_{i=1}^{val\_size}$ \Comment{生成验证集}
		\State $R_{best} \leftarrow -\infty$ \Comment{最佳奖励追踪器}
		\For{$e = 0$ \textbf{到} $N_{epoch} - 1$}
		\State $\mathcal{D}_{train} \leftarrow \{x_i\}_{i=1}^{epoch\_size}$ \Comment{生成训练集}
		\State $\{\mathcal{B}_j\}_{j=1}^{n_B} \leftarrow \text{批量划分}(\mathcal{D}_{train}, B)$
		\State $M_{\theta} \leftarrow M_{\theta}.\text{train}()$ \Comment{设置为训练模式}

		\For{$\mathcal{B} \in \{\mathcal{B}_j\}_{j=1}^{n_B}$}
		\State $\theta \leftarrow \text{批次训练}(M_{\theta}, B_{model}, \mathcal{O}, \mathcal{B})$ \Comment{见算法2}
		\EndFor

		\State $M_{\theta}.\text{decode\_type} \leftarrow \text{"贪婪"}$
		\State $M_{\theta} \leftarrow M_{\theta}.\text{eval}()$
		\State $C_{val} \leftarrow \text{评估}(M_{\theta}, \mathcal{D}_{val})$
		\State $\bar{r} \leftarrow -\frac{1}{|\mathcal{D}_{val}|}\sum_{i=1}^{|\mathcal{D}_{val}|} C_{val,i}$ \Comment{平均奖励}

		\State $B_{model} \leftarrow B_{model}.\text{轮次回调}(M_{\theta}, e)$
		\State $\alpha \leftarrow \mathcal{S}(\alpha)$ \Comment{更新学习率}

		\If{$e \bmod k_{save} = 0 \lor e = N_{epoch} - 1$}
		\State $\text{保存检查点}(\theta, \mathcal{O}, B_{model}, e)$
		\EndIf

		\If{$e = 0 \lor \bar{r} > R_{best}$}
		\State $\theta^* \leftarrow \theta$ \Comment{保存最佳模型参数}
		\State $R_{best} \leftarrow \bar{r}$
		\EndIf
		\EndFor
		\State \Return $\theta^*$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\caption{GRP批次训练子程序}
		\label{alg:grp_batch_training}
		\Require $M_{\theta}$（模型），$B_{model}$（基线模型），$\mathcal{O}$（优化器），$\mathcal{B}$（批次数据）
		\Ensure 更新后的模型参数$\theta$

		\State $(x, b_v) \leftarrow B_{model}.\text{解包批次}(\mathcal{B})$
		\State $x \leftarrow x.\text{to}(\text{设备})$, $b_v \leftarrow b_v.\text{to}(\text{设备})$ 若 $b_v \neq \text{None}$ 否则 $\text{None}$

		\State $(C, \log p) \leftarrow M_{\theta}(x)$ \Comment{$C$：成本，$\log p$：对数概率}
		\State $r \leftarrow -C$ \Comment{$r \in \mathbb{R}^{B}$：奖励}

		\If{$\text{problem\_type} = \text{"grp"}$}
		\State $A_{repair} \leftarrow \begin{cases}
				x.\text{总修复面积} & \text{若存在}                 \\
				-C / \alpha_{r}              & \text{否则（近似值）}
			\end{cases}$
		\EndIf

		\State $(b_v, \mathcal{L}_{b}) \leftarrow \begin{cases}
				B_{model}.\text{评估}(x, C) & \text{若 } b_v = \text{None} \\
				(b_v, 0)                    & \text{否则}
			\end{cases}$

		\State $\mathcal{L}_{R} \leftarrow \frac{1}{B}\sum_{i=1}^{B}(C_i - b_{v,i}) \cdot \log p_i$
		\State $\mathcal{L} \leftarrow \mathcal{L}_{R} + \mathcal{L}_{b}$ \Comment{总损失}

		\State $\nabla\theta \leftarrow \mathbf{0}$ \Comment{梯度清零}
		\State $\nabla\theta \leftarrow \nabla_{\theta}\mathcal{L}$ \Comment{计算梯度}
		\State $\|\nabla\theta\|_2 \leftarrow \min(\|\nabla\theta\|_2, g_{max})$ \Comment{裁剪梯度范数}
		\State $\theta \leftarrow \mathcal{O}(\theta, \nabla\theta)$ \Comment{更新参数}
		\State \Return $\theta$
	\end{algorithmic}
\end{algorithm}
% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 		\caption{Main Training Procedure for GRP}
% 		\label{alg:grp_main_training}
% 		\Require $\theta \in \Theta$ (model parameters), $d_e \in \mathbb{N}$ (embedding dimension), $d_h \in \mathbb{N}$ (hidden dimension), $n_{layers} \in \mathbb{N}$ (encoder layers), $n_{heads} \in \mathbb{N}$ (attention heads), $\alpha \in \mathbb{R}^{+}$ (learning rate), $N_{epoch} \in \mathbb{N}$ (training epochs), $B \in \mathbb{N}$ (batch size), $G_{size} \in \mathbb{N}$ (graph size)
% 		\Ensure $\theta^* \in \Theta$ (optimal parameters)
% 		\State $M_{\theta} \leftarrow \text{InitializeModel}(d_e, d_h, n_{layers}, n_{heads}, \theta)$
% 		\State $B_{model} \leftarrow \text{InitializeBaseline}()$
% 		\State $\mathcal{O} \leftarrow \text{Adam}(\{M_{\theta}, B_{model}\}, \alpha)$
% 		\State $\mathcal{S} \leftarrow \text{LRScheduler}(\alpha)$
% 		\State $\mathcal{D}_{val} \leftarrow \{x_i\}_{i=1}^{val\_size}$ \Comment{生成验证数据集}
% 		\State $R_{best} \leftarrow -\infty$ \Comment{最佳奖励追踪器}
% 		\For{$e = 0$ \textbf{to} $N_{epoch} - 1$}
% 		\State $\mathcal{D}_{train} \leftarrow \{x_i\}_{i=1}^{epoch\_size}$ \Comment{生成训练实例}
% 		\State $\{\mathcal{B}_j\}_{j=1}^{n_B} \leftarrow \text{BatchSplit}(\mathcal{D}_{train}, B)$
% 		\State $M_{\theta} \leftarrow M_{\theta}.\text{train}()$ \Comment{设置模型为训练模式}

% 		\For{$\mathcal{B} \in \{\mathcal{B}_j\}_{j=1}^{n_B}$}
% 		\State $\theta \leftarrow \text{TrainBatch}(M_{\theta}, B_{model}, \mathcal{O}, \mathcal{B})$ \Comment{算法2}
% 		\EndFor

% 		\State $M_{\theta}.\text{decode\_type} \leftarrow \text{"greedy"}$
% 		\State $M_{\theta} \leftarrow M_{\theta}.\text{eval}()$
% 		\State $C_{val} \leftarrow \text{Evaluate}(M_{\theta}, \mathcal{D}_{val})$
% 		\State $\bar{r} \leftarrow -\frac{1}{|\mathcal{D}_{val}|}\sum_{i=1}^{|\mathcal{D}_{val}|} C_{val,i}$ \Comment{平均奖励}

% 		\State $B_{model} \leftarrow B_{model}.\text{epoch\_callback}(M_{\theta}, e)$
% 		\State $\alpha \leftarrow \mathcal{S}(\alpha)$ \Comment{更新学习率}

% 		\If{$e \bmod k_{save} = 0 \lor e = N_{epoch} - 1$}
% 		\State $\text{SaveCheckpoint}(\theta, \mathcal{O}, B_{model}, e)$
% 		\EndIf

% 		\If{$e = 0 \lor \bar{r} > R_{best}$}
% 		\State $\theta^* \leftarrow \theta$ \Comment{保存最佳模型参数}
% 		\State $R_{best} \leftarrow \bar{r}$
% 		\EndIf
% 		\EndFor
% 		\State \Return $\theta^*$
% 	\end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[H]
% 	\begin{algorithmic}[1]
% 		\caption{TrainBatch Subroutine for GRP}
% 		\label{alg:grp_batch_training}
% 		\Require $M_{\theta}$ (model), $B_{model}$ (baseline model), $\mathcal{O}$ (optimizer), $\mathcal{B}$ (batch data)
% 		\Ensure Updated model parameters $\theta$

% 		\State $(x, b_v) \leftarrow B_{model}.\text{unwrap\_batch}(\mathcal{B})$
% 		\State $x \leftarrow x.\text{to}(\text{device})$, $b_v \leftarrow b_v.\text{to}(\text{device})$ if $b_v \neq \text{None}$ else $\text{None}$

% 		\State $(C, \log p) \leftarrow M_{\theta}(x)$ \Comment{$C$：成本，$\log p$：对数概率}
% 		\State $r \leftarrow -C$ \Comment{$r \in \mathbb{R}^{B}$：奖励}

% 		\If{$\text{problem\_type} = \text{"grp"}$}
% 		\State $A_{repair} \leftarrow \begin{cases}
% 				x.\text{total\_repair\_area} & \text{if exists}                 \\
% 				-C / \alpha_{r}              & \text{否则（近似值）}
% 			\end{cases}$
% 		\EndIf

% 		\State $(b_v, \mathcal{L}_{b}) \leftarrow \begin{cases}
% 				B_{model}.\text{eval}(x, C) & \text{if } b_v = \text{None} \\
% 				(b_v, 0)                    & \text{otherwise}
% 			\end{cases}$

% 		\State $\mathcal{L}_{R} \leftarrow \frac{1}{B}\sum_{i=1}^{B}(C_i - b_{v,i}) \cdot \log p_i$
% 		\State $\mathcal{L} \leftarrow \mathcal{L}_{R} + \mathcal{L}_{b}$ \Comment{总损失}

% 		\State $\nabla\theta \leftarrow \mathbf{0}$ \Comment{梯度清零}
% 		\State $\nabla\theta \leftarrow \nabla_{\theta}\mathcal{L}$ \Comment{计算梯度}
% 		\State $\|\nabla\theta\|_2 \leftarrow \min(\|\nabla\theta\|_2, g_{max})$ \Comment{裁剪梯度范数}
% 		\State $\theta \leftarrow \mathcal{O}(\theta, \nabla\theta)$ \Comment{更新参数}
% 		\State \Return $\theta$
% 	\end{algorithmic}
% \end{algorithm}
本文采用算法\ref{alg:grp_main_training}和\ref{alg:grp_batch_training}所示的训练流程，将Actor-Critic方法与梯度下降相结合。训练过程中使用Adam优化器，并结合学习率调度器动态调整学习率：

\begin{equation}
	\mathcal{O} \leftarrow \text{Adam}(\{M_{\theta}, B_{model}\}, \alpha)
\end{equation}

每个训练周期中，本文生成训练和验证数据集，并进行批次训练。在算法\ref{alg:grp_batch_training}中，策略梯度损失计算如下：

\begin{equation}
	\mathcal{L}_{R} \leftarrow \frac{1}{B}\sum_{i=1}^{B}(C_i - b_{v,i}) \cdot \log p_i,
\end{equation}
其中，$b_{v,i}$ 为基线模型对状态价值的估计，$(C_i - b_{v,i})$ 构成优势函数，用于指导策略更新方向。总损失函数包括策略梯度损失和基线损失：

\begin{equation}
	\mathcal{L} \leftarrow \mathcal{L}_{R} + \mathcal{L}_{b}
\end{equation}

参数更新通过梯度下降完成，并应用梯度裁剪以确保训练稳定性：

\begin{equation}
	\|\nabla\theta\|_2 \leftarrow \min(\|\nabla\theta\|_2, g_{max})
\end{equation}

\begin{equation}
	\theta \leftarrow \mathcal{O}(\theta, \nabla\theta)
\end{equation}

训练过程中，本文保存性能最佳的模型参数 $\theta^*$，以便后续使用。模型评估采用贪婪解码策略，计算验证集上的平均奖励作为评估指标：

\begin{equation}
	\bar{r} \leftarrow -\frac{1}{|\mathcal{D}_{val}|}\sum_{i=1}^{|\mathcal{D}_{val}|} C_{val,i}
\end{equation}

通过这种训练流程，模型能够逐步学习优化多无人机草原修复的策略，实现修复面积最大化的目标。

\section{本章小结}
本章介绍了强化学习方法如何应用于草原修复决策，阐述了模型结构与Actor-Critic训练流程，为端到端自动决策奠定了技术基础。

\chapter{多无人机协同调度算法}

\section{协同修复问题描述}

在多无人机协同草原修复问题中，假设各无人机每轮巡航所携带的种子数量相同，表示为：

\begin{equation}
	Q_u = Q, \quad \forall u \in U,
\end{equation}
其中，$U$ 表示无人机集合，$Q$ 为初始携带种子重量。在修复过程中，无人机自身重量因种子播撒而减轻，导致飞行能耗动态变化。每架无人机已知自身修复地图的详细信息，包括退化区域数量、位置坐标、退化程度及待修复面积等信息。任意两点间距离采用二维平面内的欧式距离计算：

\begin{equation}
	d(v_i, v_j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}
\end{equation}

\section{协同调度机制}

本文提出的多无人机协同调度算法通过局部信息与全局信息相结合的方式实现协同。对于每架无人机，本文定义：

\begin{equation}
	M_u = \{v_1, v_2, ..., v_n\}, \quad \forall u \in U,
\end{equation}
其中，$M_u$ 为无人机 $u$ 的修复地图，包含 $n$ 个待修复区域。每个区域 $v_i$ 表示为四元组：

\begin{equation}
	v_i = (id_i, (x_i, y_i), l_i, a_i), \quad i \in \{1,2,...,n\},
\end{equation}
其中，$id_i$ 为区域编号，$(x_i, y_i)$ 为坐标位置，$l_i$ 为退化程度，$a_i$ 为可修复面积。

无人机状态集 $S_u$ 定义为：

\begin{equation}
	S_u = (p_u, E_u^{rem}, V_u^{vis}, A_u^{rep}), \quad \forall u \in U,
\end{equation}
其中，$p_u$ 表示无人机当前位置，$E_u^{rem}$ 为剩余能量，$V_u^{vis}$ 为已访问序列，$A_u^{rep}$ 为已修复面积。

为模拟无人机与地面中心交互顺序，引入信号量 $P_u$ 表示无人机完成首个待修复区域的时间：

\begin{equation}
	P_u = \frac{d(p_u, v_i^{next})}{v} + \frac{a_i^{rep}}{r},
\end{equation}
其中，$v$ 为无人机飞行速度，$r$ 为单位时间内可修复面积，$v_i^{next}$ 为下一个待修复区域。

\section{动态地图更新}

本文设计的多无人机协同算法核心在于动态调整修复地图，通过适应度函数 $\Theta(u,v_i)$ 评估无人机 $u$ 与待修复区域 $v_i$ 的匹配度：

\begin{equation}
	\Theta(u,v_i) = \alpha \cdot d(p_u, v_i) + \beta \cdot \frac{1}{l_i} + \gamma \cdot \frac{1}{a_i},
\end{equation}
其中，$\alpha$、$\beta$、$\gamma$ 为权重系数，分别调节距离、退化程度和修复面积对适应度的影响。

多无人机协同调度过程可分为以下几个关键步骤：

		1) 初始地图分配：使用 K-means 等聚类方法将待修复区域分配给各无人机，形成初始修复地图 $M_u^0$。

		2) 第一次路径规划：各无人机以当前位置为起点，计算最优路径及修复面积：

		\begin{equation}
			(P_u^1, A_u^1) = \arg\max_{P \in \mathcal{P}(M_u)} \sum_{v_i \in P} a_i^{rep},
		\end{equation}
		其中，$\mathcal{P}(M_u)$ 表示满足能量约束的所有可行路径集合，$a_i^{rep}$ 为在点 $v_i$ 的修复面积。

		3) 地图动态更新：地面中心根据各无人机当前状态，更新全局修复地图：

		\begin{equation}
			M_u^{tmp} = \{v_i | v_i \in M^{global}, u = \arg\min_{u' \in U} \Theta(u',v_i)\}
		\end{equation}

		4) 第二次路径规划：无人机基于更新后的地图再次规划路径，并计算修复面积 $A_u^2$。

		5) 地图选择决策：比较两次规划的总修复面积，选择更优方案：

\begin{equation}
	M_u =
	\begin{cases}
		M_u^{tmp}, & \text{if } \sum_{u \in U} A_u^2 \geq \sum_{u \in U} A_u^1 \\
		M_u, & \text{otherwise}
	\end{cases}
\end{equation}

\section{决策与执行流程}

无人机根据信号量优先级依次执行决策。对于信号量最小的无人机 $u^*$：

\begin{equation}
	u^* = \arg\min_{u \in U} P_u
\end{equation}

该无人机在当前位置完成修复任务后，前往下一个待修复区域：

\begin{equation}
	v_i^{next} = \arg\min_{v_i \in M_{u^*}} \Theta(u^*,v_i)
\end{equation}

同时更新状态信息：

\begin{equation}
	S_{u^*} = (v_i^{next}, E_{u^*}^{rem} - E_{flight} - E_{repair}, V_{u^*}^{vis} \cup \{v_i^{curr}\}, A_{u^*}^{rep} + a_i^{rep}),
\end{equation}
其中，$E_{flight}$ 为飞行能耗，$E_{repair}$ 为修复能耗，$v_i^{curr}$ 为当前位置。

当某架无人机的修复地图为空时，该无人机返回地面中心；当全局修复地图为空时，算法结束。完整的多无人机协同调度算法如算法 \ref{alg:multi_uav_scheduling} 所示。

\begin{algorithm}[H]
	\caption{多无人机协同调度算法}
	\label{alg:multi_uav_scheduling}
	\begin{algorithmic}[1]
		\Require 参数序列 $Parms$，无人机修复地图集合 $M_u$，无人机状态集合 $S_u$
		\Ensure 无人机访问的节点序列 $O_p$，无人机修复的面积 $O_a$，无人机剩余能量 $O_e$

		\State $M_u^i \gets \text{初始化}(M_u)$ \Comment{无人机根据初始化方法（如 K-means）分配初始地图}
		\State $P_u^i \gets \text{初始化}(P_u)$ \Comment{初始化无人机信号量以决定决策优先级}

		\While{$M_u \neq \emptyset$}
		\State $E_u^{rel} \gets \text{路径规划}(M_u, P_u^{self})$ \Comment{无人机群第一次路径规划}
		\State $\text{上报中心}(S_u, M_u, E_u^{rel})$ \Comment{无人机第一次上报中心}
		\State $M_u^{tmp} \gets \text{更新地图}(M^{global}, P_u^{self})$ \Comment{中心更新地图}
		\State $\text{下发新地图}(M_u^{P_u^{tmp}})$ \Comment{中心给无人机下发新地图}
		\State $E_u^{r2} \gets \text{路径规划}(M_u^{tmp}, P_u^{self})$ \Comment{无人机群第二次路径规划}
		\State $\text{上报中心}(E_u^{r2}, Area_u^{r2})$ \Comment{无人机第二次上报中心}

		\If{$\sum_{u=1}^U Area_u^{r2} \geq \sum_{u=1}^U Area_u^{r1}$}
		\State $\text{下发新地图}(M_u^{tmp})$ \Comment{无人机选择修复面积更多的地图}
		\State $M_u \gets M_u^{tmp}$
		\EndIf

		\State $\sigma_u^{max\_p} \gets \text{决策修复面积}(E_u, M_u)$ \Comment{信号量最优先的无人机决策修复面积}
		\State $\text{执行修复与采集}(\sigma_u^{max\_p}, C_{max\_p}, P_u^{max\_p})$ \Comment{无人机执行修复和信息采集}
		\State $\text{从地图移除}(M_u, P_u^{max\_p})$
		\State $P_u^{max\_p} \gets \text{飞往下一个点}(P_u^{benefit})$ \Comment{无人机飞往下个最优点}
		\State $\text{更新信号量}(P_u^{max\_p})$ \Comment{更新无人机信号量}
		\EndWhile

		\State $\text{返回起点}(P_u^0)$ \Comment{无人机返回起点}
	\end{algorithmic}
\end{algorithm}
% \begin{algorithm}[H]
% 	\caption{多无人机协同调度算法}
% 	\label{alg:multi_uav_scheduling}
% 	\begin{algorithmic}[1]
% 		\Require 参数序列 $Parms$，无人机修复地图集合 $M_u$，无人机状态集合 $S_u$
% 		\Ensure 无人机访问的节点序列 $O_p$，无人机修复的面积 $O_a$，无人机剩余能量 $O_e$

% 		\State $M_u^i \gets \text{Initial}(M_u)$ \Comment{无人机根据初始化方法（如 K-means）分配初始地图}
% 		\State $P_u^i \gets \text{Initial}(P_u)$ \Comment{初始化无人机信号量以决定决策优先级}

% 		\While{$M_u \neq \emptyset$}
% 		\State $E_u^{rel} \gets \text{PlaningPath}(M_u, P_u^{self})$ \Comment{无人机群第一次路径规划}
% 		\State $\text{SendToCenter}(S_u, M_u, E_u^{rel})$ \Comment{无人机第一次上报中心}
% 		\State $M_u^{tmp} \gets \text{updateMap}(M^{global}, P_u^{self})$ \Comment{中心更新地图}
% 		\State $\text{RecvToUAV}(M_u^{P_u^{tmp}})$ \Comment{中心给无人机下发新地图}
% 		\State $E_u^{r2} \gets \text{PlaningPath}(M_u^{tmp}, P_u^{self})$ \Comment{无人机群第二次路径规划}
% 		\State $\text{SendToCenter}(E_u^{r2}, Area_u^{r2})$ \Comment{无人机第二次上报中心}

% 		\If{$\sum_{u=1}^U Area_u^{r2} \geq \sum_{u=1}^U Area_u^{r1}$}
% 		\State $\text{RecvToUAV}(M_u^{tmp})$ \Comment{无人机选择修复面积更多的地图}
% 		\State $M_u \gets M_u^{tmp}$
% 		\EndIf

% 		\State $\sigma_u^{max\_p} \gets \text{DecideArea}(E_u, M_u)$ \Comment{信号量最优先的无人机决策修复面积}
% 		\State $\text{ActionUAV}(\sigma_u^{max\_p}, C_{max\_p}, P_u^{max\_p})$ \Comment{无人机执行修复和信息采集}
% 		\State $\text{DropPointFromMap}(M_u, P_u^{max\_p})$
% 		\State $P_u^{max\_p} \gets \text{FlyToPoint}(P_u^{benefit})$ \Comment{无人机飞往下个最优点}
% 		\State $\text{Update}(P_u^{max\_p})$ \Comment{更新无人机信号量}
% 		\EndWhile

% 		\State $\text{FlyToPoint}(P_u^0)$ \Comment{无人机返回起点}
% 	\end{algorithmic}
% \end{algorithm}

\section{本章小结}
本章给出了多无人机协同算法，通过两次路径规划与地图更新实现任务分配动态调整，兼顾局部优化与全局效益，为后续实验提供了多无人机协同调度框架。

\chapter{实验与分析}
本部分本文展示了一些仿真实验的具体配置与结果。
\label{sub:实验配置表格}
\section{仿真配置}
\begin{table}[H]
	\centering
	\caption{实验硬件配置}
	\begin{tabular}{ll} % 使用两列左对齐格式
		\toprule
		配置      & 描述                                                       \\
		\midrule
		CPU     & AMD Ryzen Threadripper 3970X 32-Core Processor, 3.79 GHz \\
		GPU     & NVIDIA GeForce RTX 3090 Ti                               \\
		RAM     & ADATA 192GB-DDR4                                         \\
		OS      & Ubuntu Server 22.04.3 LTS                                \\
		Python  & Python 3.9                                               \\
		PyTorch & version 1.12.1+cu113                                     \\
		\bottomrule
	\end{tabular}
	\label{tbl_hardware_config}
\end{table}
\section{实例设置}
为验证所提出模型与算法的有效性，本文在六种不同规模的草原实例上进行了仿真，草原区域边长分别为 $500$ 到 $1000$，步长为 $100$。所有实例均在二维空间内定义，任意两个待修复区域 $v_i$ 与 $v_j$ 之间的距离采用欧氏距离计算。每组实例中，待修复区域数量 $N$ 分别设置为 $60, 80, 100, 120, 140, 160$，无人机数量 $U$ 设置为 $4, 6, 8$，单架无人机的活动范围 $L$ 与草原边长一致。所有待修复区域的坐标均在对应草原区域内随机生成，退化程度 $l_i$ 服从 $[0.3, 0.8]$ 的均匀分布，基站均位于 $(0,0)$。每个待修复区域的最大可修复单位圆数量 $\sigma_i$ 随区域面积变化，范围为 $10$ 到 $35$，步长为 $5$。无人机初始能量 $E_{max}$ 与草原规模对应，具体参数如表\ref{tbl_instance_setting} 所示。

\begin{table}[H]
	\centering
	\caption{仿真实验实例设置参数表}
	\begin{tabular}{lll}
		\toprule
		参数  & 描述       & 值                               \\
		\midrule
		$N$ & 待修复区域数量  & $60, 80, 100, 120, 140, 160$    \\
		$U$ & 无人机数量    & $4, 6, 8$                       \\
		$L$ & 单无人机活动范围 & $500, 600, 700, 800, 900, 1000$ \\
		\bottomrule
	\end{tabular}
	\label{tbl_instance_setting}
\end{table}

\section{算法对比设置}

为了与研究热点中的最优化路径规划策略\cite{aggarwal2020path}进行比较，本文设置了如下对比算法：

\textbf{协同启发式与概率模型学习算法（CHAPBILM）\cite{JIAO2024108084}}：该方法将问题分解为两阶段，第一阶段采用启发式算法（如2-opt、or-opt等）优化无人机的访问顺序，第二阶段利用概率模型学习（PBIL）动态分配各区域的修复面积，并结合最大剩余能量局部搜索（MRELS）进一步提升解的质量。该方法通过协同优化路径和修复面积，充分考虑了两者的耦合关系，提升了整体修复效果。

该算法可与多无人机协同调度算法结合，进一步验证本文提出方法的有效性和优越性。
\section{参数设置}
实验仿真具体参数如下。

\label{sub:无人机参数表格}
\begin{table}[H]
	\centering
	\caption{实验软件配置}
	\begin{tabular}{lll} % 三列左对齐格式
		\toprule
		参数              & 描述                & 值                   \\
		\midrule
		\( M \)         & 无人机重量             & 1.5                 \\
		\( g \)         & 重力加速度             & 9.8                 \\
		\( \rho \)      & 空气流体密度            & 1.024               \\
		\( \zeta \)     & 无人机旋转螺旋桨面积        & 0.2                 \\
		\( h \)         & 无人机旋翼数            & 6                   \\
		\( \gamma \)    & 修复能耗正系数           & 2                   \\
		\( \eta \)      & 草原环境相关的正系数        & \( 1 \times 10^4 \) \\
		\( e_{ap} \)    & 无人机收集一个单位圆信息所需的能耗 & \( 2 \times 10^4 \) \\
		\( E_{max} \)   & 无人机所携带的最大能量       & \( 3 \times 10^6 \) \\
		\( B \)         & 强化学习模型训练时的图批次大小   & 512                 \\
		\( N_{epoch} \) & 强化学习模型训练时的回合数     & 512                 \\
		\bottomrule
	\end{tabular}
	\label{tbl_drone_parameters}
\end{table}

\section{仿真结果}


下面本文展示本文所提出方法的仿真实验结果，包括训练过程中的损失函数和修复面积变化曲线，以及不同方法的路径规划可视化对比结果。

\begin{figure}[H]
	\centering
	\includesvg[width=1\textwidth]{figures/actor_loss_curve.svg}
	\caption{深度强化学习训练过程中的损失函数变化}
	\label{fig:training_loss_curve}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[width=1\textwidth]{figures/avg_repair_area_curve.svg}
	\caption{深度强化学习训练过程中的修复面积变化}
	\label{fig:training_reward_curve}
\end{figure}

如图\ref{fig:training_loss_curve}所示，该图展示了深度强化学习模型在训练过程中的损失函数随迭代轮数的变化趋势，可以看出损失值整体呈下降趋势，表明模型逐步收敛。从曲线可观察到，当训练步数达到约500000步后，损失函数趋于稳定，波动范围显著减小，表明模型已基本收敛。图\ref{fig:training_reward_curve}展示了训练过程中平均修复面积的变化曲线，随着训练的进行，平均修复面积逐步提升，说明模型的决策能力不断增强，能够获得更优的修复方案。同样，在约500000步后，修复面积也达到相对稳定状态，进一步证实了模型的有效收敛。这两个图共同验证了所提出深度强化学习方法在多无人机草原修复任务中的有效性和收敛性。
\begin{figure}[H]
	\centering
	\includesvg[width=0.99\textwidth]{figures/RL_combined_uav_routes.svg}
	\caption{基于深度强化学习的多无人机草原修复的最优路径规划图}
	\label{fig:RL_combined_uav_routes}
\end{figure}

图\ref{fig:RL_combined_uav_routes}展示了基于深度强化学习方法的多无人机草原修复路径规划结果。可以看到，深度强化学习方法能够为每架无人机分配合理的修复区域，路径交叉较少，任务分配均衡，有效减少了无人机间的干扰。尤其在待修复区域分布密集时，算法能够根据能量约束和区域分布动态优化任务分配，使各无人机负载更为均衡。

\begin{figure}[H]
	\centering
	\includesvg[width=0.99\textwidth]{figures/ILS_combined_uav_routes.svg}
	\caption{基于迭代局部搜索的多无人机草原修复的最优路径规划图}
	\label{fig:ILS_combined_uav_routes}
\end{figure}

图\ref{fig:ILS_combined_uav_routes}为迭代局部搜索方法的多无人机路径规划结果。与深度强化学习方法相比，迭代局部搜索在路径分配上表现出一定局限性，部分无人机路径存在较多长距离飞行和交叉，导致能量消耗增加，任务分配不够灵活。该方法在区域分配上较为机械，难以充分适应区域分布和能耗的动态变化。

Gap(\%) 计算公式为 $(\text{DRL} - \text{CHAPBILM}) / \text{CHAPBILM} \times 100\%$。路径长度 Gap 为负值表示 DRL 更优，修复面积 Gap 为正值表示 DRL 更优。
\begin{table}[H]
	\centering
	\caption{路径长度与修复面积对比（DRL 与 CHAPBILM ）}
	\small
	\setlength{\tabcolsep}{3.5pt}
	\begin{tabular}{cccc ccc ccc}
		\toprule
		区域数                 & 草原边长                 & 无人机数 &  & \multicolumn{3}{c}{路径长度} & \multicolumn{3}{c}{修复面积}                                       \\
		\cmidrule(lr){5-7} \cmidrule(lr){8-10}
							&                      &      &  & DRL                      & CHAPBILM                       & Gap(\%) & DRL    & CHAPBILM     & Gap(\%) \\
		\midrule
		\multirow{9}{*}{60} & \multirow{3}{*}{500} & 4    &  & 9396.35                  & 12648.21                 & \textbf{-25.68}  & 267.00 & 194.00 & \textbf{37.63}   \\
							&                      & 6    &  & 21348.69                 & 19110.48                 & 11.71   & 258.00 & 205.00 & \textbf{25.85}   \\
							&                      & 8    &  & 26118.19                 & 27145.66                 & \textbf{-3.79}   & 304.00 & 218.00 & \textbf{39.45}   \\
		\cmidrule(lr){2-10}
							& \multirow{3}{*}{600} & 4    &  & 13339.68                 & 16255.96                 & \textbf{-17.92}  & 271.00 & 223.00 & \textbf{21.52}   \\
							&                      & 6    &  & 26523.73                 & 27174.68                 & \textbf{-2.39}   & 294.00 & 206.00 & \textbf{42.72}   \\
							&                      & 8    &  & 31186.17                 & 31494.13                 & \textbf{-0.98}   & 257.00 & 224.00 & \textbf{14.73}   \\
		\cmidrule(lr){2-10}
							& \multirow{3}{*}{700} & 4    &  & 17622.78                 & 22103.46                 & \textbf{-20.27}  & 282.00 & 291.00 & -3.09   \\
							&                      & 6    &  & 28252.23                 & 28539.91                 & \textbf{-1.01}   & 270.00 & 218.00 & \textbf{23.85}   \\
							&                      & 8    &  & 37585.42                 & 36859.35                 & 1.97    & 238.00 & 254.00 & -6.30   \\
		\midrule
		\multirow{9}{*}{80} & \multirow{3}{*}{500} & 4    &  & 10221.88                 & 17217.03                 & \textbf{-40.60}  & 370.00 & 315.00 & \textbf{17.46}   \\
							&                      & 6    &  & 22135.76                 & 25252.45                 & \textbf{-12.37}  & 375.00 & 240.00 & \textbf{56.25}   \\
							&                      & 8    &  & 30491.72                 & 31922.23                 & \textbf{-4.48}   & 306.00 & 267.00 & \textbf{14.61}   \\
		\cmidrule(lr){2-10}
							& \multirow{3}{*}{600} & 4    &  & 15973.03                 & 22825.91                 & \textbf{-30.02}  & 363.00 & 314.00 & \textbf{15.61}   \\
							&                      & 6    &  & 29742.35                 & 28044.73                 & 6.05    & 406.00 & 282.00 & \textbf{44.00}   \\
							&                      & 8    &  & 39943.98                 & 40845.86                 & \textbf{-2.21}   & 399.00 & 269.00 & \textbf{48.33}   \\
		\cmidrule(lr){2-10}
							& \multirow{3}{*}{700} & 4    &  & 22577.68                 & 26113.54                 & \textbf{-13.56}  & 335.00 & 267.00 & \textbf{25.47}   \\
							&                      & 6    &  & 33566.63                 & 34738.58                 & \textbf{-3.38}   & 380.00 & 298.00 & \textbf{27.52}   \\
							&                      & 8    &  & 44308.24                 & 42493.45                 & 4.28    & 355.00 & 311.00 & \textbf{14.14}   \\
		\bottomrule
	\end{tabular}
	\label{tab:combined_comparison}
\end{table}

由表~\ref{tab:combined_comparison} 可以看出，深度强化学习（DRL）方法在大多数测试场景下均优于模拟退火（SA）算法。在路径长度方面，DRL 在多数情况下实现了更短的总路径，部分场景下路径长度缩短超过 20\%。在修复面积方面，DRL 也表现出显著提升，提升幅度普遍在 15\% 以上，最高可达 56\%。尤其在无人机数量较少或区域分布密集时，DRL 能更好地协调多无人机任务分配，实现更高的修复效率。总体来看，DRL 在路径优化和面积最大化两个方面均优于传统启发式算法，显示出其在复杂多无人机协同任务中的有效性和优势。

% Required packages: \usepackage{booktabs}, \usepackage{float}, \usepackage{multirow}

\section{本章小结}
本章通过实验验证了方法的有效性。结果表明，深度强化学习在路径规划和修复面积方面优于传统算法，展示了应用潜力。

\chapter{总结与展望}

\section{总结}
本文提出了结合深度强化学习方法求解的多无人机协同草原修复方法，利用深度强化学习的序列决策能力，构建编码器-解码器架构深度神经网络模型，使用强化学习Actor-Critic方法对模型进行训练，无需人为设计即可自动学习出优秀的策略。深度神经网络模型在编码器中的多头注意力层对待修复区域个体特征和待修复区域位置特征进行信息交互，通过自回归解码器输出下一步可能的修复方案。此外，本文还提出了多无人机协同的修复算法，在充分利用已有强化学习方法的基础上进一步优化求解效果。

实验结果表明，本文提出的算法优于基于贪婪策略的双层决策算法，且随着问题规模的增大优势更为明显；相较于贪婪策略，本文提出的单无人机训练、多无人机协同的策略具有更强寻优能力，在规定时间内进一步提升了求解效果，展示出神经网络算法强大的泛化与寻优能力，同时为求解大规模草原修复问题提供了新的思路和方法。

\section{展望}
在仿真结果与分析部分，由于项目时间限制，其中仍有众多方面可以值得扩展研究。例如：在给定的参数范围内，选择更多的启发式算法作为对比算法，从而能对控制算法的性能有更公正的评估。或在本文提出的控制算法框架下，通过控制变量法改变草场规模、无人机初始能量 $E_{max}$ 等参数多次训练神经网络，而后通过大量的随机模拟取最佳、最差及平均等指标，来探讨算法的稳定性。


%论文后部
\backmatter
%=======%
%引入参考文献文件
%=======%
\printbib
% \nocite{*} %显示数据库中有的，但是正文没有引用的文献

% \Appendix

% 这里是附录页，附上你的程序或必要的相关知识

% {\bfseries 编译方式:}

\Thanks

在毕业论文完成之际，我想向所有在我学习和研究过程中给予帮助和支持的人表达我诚挚的感谢。

首先，我要衷心感谢我的导师焦栋斌老师。在整个研究过程中，焦老师以其渊博的学识、严谨的治学态度和敏锐的科研洞察力给予我悉心指导。他不仅在学术上为我提供了宝贵的建议和方向，而且在科研思维和方法论上也给了我深刻的启发。没有焦老师的悉心指导和鼓励，我难以完成这项研究工作。

特别感谢学长袁雨辰、刘波和王凌宇，本研究工作是在他们先前研究的基础上进行的改进和拓展。他们的开创性工作为我提供了重要的理论基础和技术支持，使我能够站在巨人的肩膀上继续探索。感谢他们在我研究过程中给予的宝贵建议、经验分享和无私帮助。

感谢课题组的其他成员。我们的头脑风暴和技术讨论不仅促进了研究的顺利进行，也让我对问题有了更加深入的理解。

感谢信息科学与工程学院的各位老师，在我本科学习期间传授知识、培养能力，为我打下了坚实的专业基础。特别感谢参与我论文评审和答辩的各位老师，您们的建议和意见使我的论文更加完善。

感谢实验室的师兄师姐和同学们，在日常学习和研究中的讨论和交流，让我受益匪浅。你们的陪伴和支持，使我的研究生活充满乐趣和动力。

感谢我的家人，是你们无条件的支持和理解，让我能够专心致志地投入学习和研究。你们是我坚强的后盾和不断前行的动力源泉。

最后，感谢所有在我成长道路上给予过帮助的人，正是因为有了你们的支持与鼓励，我才能顺利完成学业，迎接人生的新挑战。

\Grade %这一句才是成绩页，上面是填写

\end{document}
