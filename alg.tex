\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\caption{GRP主训练流程}
		\label{alg:grp_main_training}
		\Require $\theta \in \Theta$（模型参数），$d_e \in \mathbb{N}$（嵌入维度），$d_h \in \mathbb{N}$（隐藏层维度），$n_{layers} \in \mathbb{N}$（编码器层数），$n_{heads} \in \mathbb{N}$（注意力头数），$\alpha \in \mathbb{R}^{+}$（学习率），$N_{epoch} \in \mathbb{N}$（训练轮数），$B \in \mathbb{N}$（批次大小），$G_{size} \in \mathbb{N}$（图规模）
		\Ensure 最优参数 $\theta^* \in \Theta$
		\State $M_{\theta} \leftarrow \text{初始化模型}(d_e, d_h, n_{layers}, n_{heads}, \theta)$
		\State $B_{model} \leftarrow \text{初始化基线模型}()$
		\State $\mathcal{O} \leftarrow \text{Adam优化器}(\{M_{\theta}, B_{model}\}, \alpha)$
		\State $\mathcal{S} \leftarrow \text{学习率调度器}(\alpha)$
		\State $\mathcal{D}_{val} \leftarrow \{x_i\}_{i=1}^{val\_size}$ \Comment{生成验证集}
		\State $R_{best} \leftarrow -\infty$ \Comment{最佳奖励追踪器}
		\For{$e = 0$ \textbf{到} $N_{epoch} - 1$}
		\State $\mathcal{D}_{train} \leftarrow \{x_i\}_{i=1}^{epoch\_size}$ \Comment{生成训练集}
		\State $\{\mathcal{B}_j\}_{j=1}^{n_B} \leftarrow \text{批量划分}(\mathcal{D}_{train}, B)$
		\State $M_{\theta} \leftarrow M_{\theta}.\text{train}()$ \Comment{设置为训练模式}

		\For{$\mathcal{B} \in \{\mathcal{B}_j\}_{j=1}^{n_B}$}
		\State $\theta \leftarrow \text{批次训练}(M_{\theta}, B_{model}, \mathcal{O}, \mathcal{B})$ \Comment{见算法\ref{alg:grp_batch_training}}
		\EndFor

		\State $M_{\theta}.\text{decode\_type} \leftarrow \text{"贪婪"}$
		\State $M_{\theta} \leftarrow
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \begin{algorithmic}[1]
        \caption{GRP批次训练子程序}
        \label{alg:grp_batch_training}
        \Require $M_{\theta}$（模型），$B_{model}$（基线模型），$\mathcal{O}$（优化器），$\mathcal{B}$（批次数据）
        \Ensure 更新后的模型参数 $\theta$
        
        \State $(x, b_v) \leftarrow B_{model}.\text{unwrap\_batch}(\mathcal{B})$ \Comment{解包批次数据}
        \State $x \leftarrow x.\text{to}(\text{device})$ \Comment{将数据移至计算设备}
        \State $b_v \leftarrow b_v \neq \text{None} ? b_v.\text{to}(\text{device}) : \text{None}$
        
        \State $(C, \log p, \pi, A_{repair}) \leftarrow M_{\theta}(x)$ \Comment{前向传播获取成本、对数概率、路径和修复面积}
        \State $r \leftarrow -C$ \Comment{计算奖励值}
        
        \If{$b_v = \text{None}$}
            \State $(b_v, \mathcal{L}_{b}) \leftarrow B_{model}.\text{evaluate}(x, C)$ \Comment{评估基线值}
        \Else
            \State $\mathcal{L}_{b} \leftarrow 0$ \Comment{使用给定的基线值}
        \EndIf
        
        \State $\mathcal{L}_{R} \leftarrow ((C - b_v) \cdot \log p).\text{mean}()$ \Comment{计算策略梯度损失}
        \State $\mathcal{L} \leftarrow \mathcal{L}_{R} + \mathcal{L}_{b}$ \Comment{计算总损失}
        
        \State $\mathcal{O}.\text{zero\_grad}()$ \Comment{清空梯度}
        \State $\mathcal{L}.\text{backward}()$ \Comment{反向传播}
        
        \State $has\_bad\_grad \leftarrow \text{False}$ \Comment{检查梯度是否包含NaN/Inf}
        \For{$p \in M_{\theta}.\text{parameters}()$}
            \If{$p.\text{grad} \neq \text{None}$ 且 ($\text{isnan}(p.\text{grad})$ 或 $\text{isinf}(p.\text{grad})$)}
                \State $has\_bad\_grad \leftarrow \text{True}$
                \State $p.\text{grad} \leftarrow \text{nan\_to\_num}(p.\text{grad}, 0)$ \Comment{替换无效梯度为0}
            \EndIf
        \EndFor
        
        \State $\|\nabla\theta\| \leftarrow \text{clip\_grad\_norms}(\mathcal{O}, g_{max})$ \Comment{梯度裁剪}
        
        \If{$\text{not}\ has\_bad\_grad$}
            \State $\theta \leftarrow \mathcal{O}.\text{step}()$ \Comment{参数更新}
        \EndIf
        
        \State \Return $\theta$ \Comment{返回更新后的参数}
    \end{algorithmic}
\end{algorithm}